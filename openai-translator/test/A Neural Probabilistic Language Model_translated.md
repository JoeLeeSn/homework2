《机器学习研究杂志》3(2003)1137–1155 投稿日期4/02；出版日期2/03
神经概率语言模型
Yoshua Bengio BENGIOY@IRO.UMONTREAL.CA
Réjean Ducharme DUCHARME@IRO.UMONTREAL.CA
Pascal Vincent VINCENTP@IRO.UMONTREAL.CA
Christian Jauvin JAUVINC@IRO.UMONTREAL.CA
蒙特利尔大学信息与运筹学系
数学研究中心
加拿大魁北克蒙特利尔
编辑：Jaz Kandola，Thomas Hofmann，Tomaso Poggio和John Shawe-Taylor
摘要
统计语言建模的目标是学习语言中单词序列的联合概率函数。这在本质上是困难的，因为维度的诅咒：测试模型的单词序列很可能与训练过程中出现的所有单词序列不同。传统但非常成功的方法是基于n-gram，并通过连接在训练集中见过的非常短的重叠序列来获得泛化。我们提出通过学习一种分布式表示来对抗维度的诅咒，这种表示允许每个训练句子向模型提供有关大量语义相关句子的信息。该模型同时学习每个单词的分布式表示以及单词序列的概率函数，以这些表示的形式表达。通过这种方式，当组成一个以前未见过的单词序列的单词类似（在具有相似表示的意义上）于构成一个已经见过的句子时，该序列获得高概率。在合理的时间内训练这样大型的模型（具有数百万个参数）本身就是一个重大挑战。我们报告了使用神经网络进行概率函数的实验，在两个文本语料库上显示出所提出的方法显著改善了当前n-gram模型，并且所提出的方法允许利用更长的上下文。
关键词：统计语言建模，人工神经网络，分布式表示，维度的诅咒
1.引言
一种使语言建模和其他学习问题困难的基本问题是维度的诅咒。当我们想要对许多离散随机变量之间的联合分布进行建模时（例如句子中的单词或数据挖掘任务中的离散属性），这一问题尤为明显。例如，如果我们想要对自然语言中具有大小为100,000的词汇V的10个连续单词的联合分布进行建模，那么潜在的自由参数可能有100000^10−1 = 10^50−1个。在建模连续变量时，我们可以更容易地获得泛化（例如，使用光滑函数类，如多层神经网络或高斯混合模型），因为可以预期要学习的函数具有一些局部平滑性质。对于离散空间，泛化结构并不那么明显：这些离散变量的任何变化可能会对函数的值产生重大影响。
转载请注明出处。【特此声明：以上内容仅供参考，如有任何不足，欢迎指正！感谢！】

---

本文探讨了BENGIO, DUCHARME, VINCENT AND JAUVIN，当每个离散变量可以取的值很多时，大多数观测对象在汉明距离中几乎彼此相距甚远。
一种有用的方法来可视化不同学习算法如何泛化，受到非参数密度估计的启发，就是思考一开始集中在训练点（例如训练句子）上的概率质量是如何分布在更大的体积中的，通常是在训练点周围的某种形式的邻域内。在高维空间中，关键是将概率质量分布在重要的地方，而不是均匀地在每个训练点周围的所有方向。本文将展示这里提出的方法如何泛化，与先前现有的统计语言建模方法泛化的方式有根本不同。
语言的统计模型可以通过给定所有之前的单词来表示下一个单词的条件概率，即
T
Pˆ(wT)=(cid:213) Pˆ(w jwt−1),
1 t 1
t=1
其中 w
t
是第 t 个单词，写子序列 w ij = (w i,w i+1,(cid:1)(cid:1)(cid:1),w j−1,w j)。这种统计语言模型已经在许多涉及自然语言的技术应用中被证明是有用的，例如语音识别、语言翻译和信息检索。因此，改进统计语言模型可能会对这些应用产生重要影响。
建立自然语言的统计模型时，通过利用单词顺序和临时更接近的单词在单词序列中统计上更相关的事实，极大地减少了建模问题的困难。因此，n-gram模型为每个大量上下文（即最后n-1个单词的组合）构建下一个单词的条件概率表。
我们只考虑训练语料库中实际出现或出现频率足够高的连续单词组合。当在训练语料库中出现一个新的 n 个单词组合时会发生什么？我们不希望为这种情况分配零概率，因为这种新组合很可能会发生，并且对于更大的上下文大小来说它们会更加频繁地发生。一个简单的答案是查看使用较小的上下文大小预测的概率，就像在后退三元模型（Katz, 1987）或平滑（或插值）三元模型（Jelinek and Mercer, 1980）中所做的那样。因此，在这些模型中，如何从训练语料库中看到的单词序列的泛化基本上是如何到达新的单词序列的？理解这是如何发生的一种方法是考虑与这些插值或后退 n-gram 模型对应的生成模型。基本上，一个新的单词序列是通过“粘合”在训练数据中频繁出现的非常短且重叠的长度为 1、2... 或最多 n 个单词的部件生成的。获得下一个部分的概率的规则隐含在后退或插值 n-gram 算法的细节中。通常研究人员使用 n =3，即三元组，并获得最先进的结果，但请参见 Goodman（2001）有关如何结合许多技巧可以产生重大改进的信息。显然，与仅仅是前面的几个单词的身份相比，紧接着要预测的单词之前的序列中有更多的信息。
在这种方法中至少有两个特点值得改进，我们将在接下来的研究中深入探讨。

---

《神经概率语言模型》本文将关注的是本文中将要讨论的。首先，它没有考虑超过1或2个单词的上下文，其次它没有考虑单词之间的“相似性”。例如，在训练语料库中看到句子“The cat is walking in the bedroom”应该帮助我们推广到几乎同样可能的句子“A dog was running in a room”，仅仅因为“dog”和“cat”（以及“the”和“a”，“room”和“bedroom”等等）具有类似的语义和语法角色。
有许多方法被提出来解决这两个问题，我们将在第1.2节简要解释这里提出的方法与一些早期方法之间的关系。我们将首先讨论提议方法的基本思想。在第2节中将介绍更形式化的展示，使用一个依赖于共享参数多层神经网络的这些想法的实现。本文的另一个贡献涉及训练这样非常大的神经网络（具有数百万参数）用于非常大的数据集（具有数百万或数千万个例子）的挑战。最后，本文的一个重要贡献是表明训练这样大规模模型是昂贵但可行的，适用于大数据集，并产生良好的比较结果（第4节）。
本文中的许多操作都是采用矩阵表示，小写字母v表示一列向量和v的转置，A表示矩阵A的第j行，x.y=x0 y。
1.1 用分布式表示解决维度诅咒
简言之，提出方法的思想可以总结如下：
1. 为词汇表中的每个单词关联一个分布式词特征向量（一个在Rm中的实值向量），
2. 用这些单词在序列中的特征向量表示单词序列的联合概率函数，并
3. 同时学习单词特征向量和该概率函数的参数。
特征向量代表单词的不同方面：每个单词与向量空间中的一个点相关联。特征的数量（例如，在实验中 m=30，60或100）要远远小于词汇表的大小（例如，17000个）。概率函数表示为下一个单词在给定前几个单词的条件下的条件概率的乘积（例如，在实验中使用多层神经网络来预测在给定前几个单词的情况下的下一个单词）。这个函数有参数，可以通过迭代调整来最大化训练数据的对数似然或一个正则化准则（例如通过添加权值衰减惩罚）。与每个单词关联的特征向量是通过学习获得的，但也可以使用先验的语义特征进行初始化。
为什么它有效？在前面的例子中，如果我们知道dog和cat在语义和句法上扮演着相似的角色，同样对于（the，a），（bedroom，room），（is，was），

---

本文提出的模型将自然泛化（即从The cat is walking in the bedroom到A dog was running in a room，以及类似地从The cat is running in a room和A dog is walking in a bedroom，The dog was walking in the room等组合中转移概率质量），因为“相似”的词汇预计会具有相似的特征向量，并且因为概率函数是这些特征值的平滑函数，对特征的微小变化将导致概率的微小变化。因此，在训练数据中只出现上述句子之一将增加该句子的概率，不仅如此，还会增加其在句子空间中的“邻居”的组合数量的概率（表示为特征向量序列）。

与先前工作的关系：
利用神经网络建模高维离散分布的想法已被证明对学习Z（1）、...、Z（n）的联合概率很有用，其中每一个都可能是不同性质的随机变量（Bengio和Bengio，2000a,b）。在该模型中，联合概率被分解为条件概率的乘积
P̂（Z1=z1, ..., Zn=zn）=ΠiP̂（Zi=zi|Zi-1=zi-1, Z1=z1），
其中g（.）是由具有特殊从左到右结构的神经网络表示的函数，第i个输出块g（）计算用于表示在具有任意顺序的以前的Z值之后给出Z的条件分布的参数。对四组UCI数据集的实验表明这种方法运转得相当好（Bengio和Bengio，2000a,b）。现在我们必须处理变长数据，例如句子，所以上述方法必须进行调整。另一个重要的区别是，这里所有的Z（在第i个位置的单词）都指向相同类型的对象（一个词）。因此，这里提出的模型引入了参数在时间上的共享 - 相同的g在时间上使用 - 以及在不同位置输入单词。这是同一概念的成功大规模应用，连同在连接主义早期就被提倡过的为符号化数据学习分布表示的（旧）想法（Hinton，1986年，Elman，1990年）。更近期，Hinton的方法得到了改进，并成功地在学习几个符号关系上进行了证明（Paccanaro和Hinton，2000）。利用神经网络进行语言建模的想法也不是新的（例如Miikkulainen和Dyer，1991）。与此相反，我们将这个想法推广到大规模，并集中于学习单词序列的分布统计模型，而不是学习单词在句子中的作用。这里提出的方法也与使用神经网络对基于字符的文本压缩的先前提议相关，用于预测下一个字符的概率（Schmidhuber，1996）。使用神经网络进行语言建模的想法也被Xu和Rudnicky（2000）独立提出，尽管实验使用的网络没有隐藏单元，并且只有一个输入词，这限制了模型基本上捕获了一元和二元统计。

从训练序列到新序列中获取一些单词之间的相似性，以获得泛化的想法并不是新的。例如，它在基于学习单词聚类的方法中被利用（Brownetal。，1992，Pereiraetal。，1993，Niesleretal。，1998，Baker1140）。

---

神经概率语言模型(McCallum，1998)：每个单词与一个离散类别相关联，确定性地或者概率性地，同一类别中的单词在某些方面是相似的。在这里提出的模型中，我们不再用离散的随机或确定性变量来表征相似性（这对应于单词集的软分区或硬分区），而是使用一个连续实向量来表示每个单词，即学习到的分布特征向量，以表示单词之间的相似性。本文中的实验比较包括基于类别的n-grams的结果（Brown等，1992年，Ney和Kneser，1993年，Niesler等，1998年）。
在信息检索领域（例如Schutze，1993年的工作）中，已经很好地利用了将单词表示为向量空间的概念，其中单词的特征向量是基于它们在同一文档中共同出现的概率来学习的（潜在语义索引，参见Deerwester等人，1990年）。一个重要的区别是，这里我们寻找一个有助于紧凑地表示自然语言文本中单词序列的概率分布的表示。实验表明，联合学习表示（单词特征）和模型是非常有用的。我们尝试（但没有成功）使用固定的单词特征表示每个单词，这些特征是与单词w周围文本中出现的单词的共现频率的第一个主成分相关的。这与LSI用于信息检索中对文档的做法类似。提出使用连续表示单词的概念在n-gram统计语言模型中成功利用了Bellegarda（1997）的方法，使用LSI动态地识别话题。
在神经网络中，将符号表示为向量空间的思想以前也曾以参数共享层的形式出现过（例如，Riis和Krogh，1996年，用于二级结构预测，以及Jensen和Riis，2000年用于文本到语音的映射）。
2.神经模型
训练集是一个单词序列w (cid:1)(cid:1)(cid:1)w，其中w∈V，词汇表V是一个庞大但有限的集合。目标是学习一个良好的模型f(w t,(cid:1)(cid:1)(cid:1),w t−n+1)=Pˆ(w t|w t 1−1)，即在样本外似然性较高。下面，我们报告了1/Pˆ(w t|w t−1)的几何平均值，也称为困惑度，它也是平均负对数似然性的指数。模型的唯一约束是对于任何选择的wt 1−1，(cid:229) j∈V，f(i，w t−1，(cid:1)(cid:1)(cid:1)，w t−n+1)=1，其中f>0。通过这些条件概率的乘积，可以得到单词序列的联合概率模型。
我们将函数f(w t,(cid:1)(cid:1)(cid:1),w t−n+1)=Pˆ(w t|w t 1−1)分解成两部分：
1. 从V中的任何元素i到实向量C(i)∈Rm的映射C。它代表词汇表中每个单词关联的分布特征向量。在实践中，C由一个自由参数的ajVj×m矩阵表示。
2. 通过C表达的单词的概率函数：一个函数g将上下文中单词的特征向量序列(C(w t−n+1)，(cid:1)(cid:1)(cid:1)，C(w t−1))映射到V中下一个单词w的条件概率分布。g的输出是一个向量，其第i个元素估计了Pˆ(w=ijw t−1)，如图1所示。
f(i,w t−1,(cid:1)(cid:1)(cid:1),w t−n+1)=g(i,C(w t−1),(cid:1)(cid:1)(cid:1),C(w t−n+1))
函数f是这两个映射(C和g)的复合，其中C在整个上下文中是共享的。每个部分都与一些参数相关联。

---

BENGIO, DUCHARME, VINCENT AND JAUVIN
第i输出=P(w = ij|context)
t
softmax
. . . . . .
大部分计算在这里
双曲正切
. . . . . .
C(w t−n+1) C(w t−2) C(w t−1)
. . . . . . . . . . . .
表格矩阵C
查找
共享参数
在C中
跨单词
w t−n+1的索引 w t−2的索引 w t−1的索引
图1：神经架构：f(i,w t−1,(cid:1)(cid:1)(cid:1),w t−n+1)=g(i,C(w t−1),(cid:1)(cid:1)(cid:1),C(w t−n+1))，其中g是神经网络，C(i)是第i个单词的特征向量。
映射C的参数只是特征向量本身，由矩阵C表示，其行i是单词i的特征向量C(i)。函数g可以通过前馈或循环神经网络或另一个参数化函数来实现，带有参数w。整体参数集合为q =(C,w)。
训练通过寻找最大化训练语料的对数似然率的q来实现：
L= 1 (cid:229) log f(w t,w t−1,(cid:1)(cid:1)(cid:1),w t−n+1;q )+R(q)，
t
其中 R(q) 是一个正则项。例如，在我们的实验中，R是一个只应用于神经网络权重和C矩阵的权重衰减惩罚，而不应用于偏差。
在上述模型中，自由参数的数量与词汇表中单词的数量V以及顺序n线性相关。如果引入更多的共享结构，例如使用时延神经网络或循环神经网络（或两者的组合），则缩放因子可能会减小到亚线性。
在下面的大多数实验中，神经网络除了词特征映射之外还有一个隐藏层，并且可以选择将词特征直接连接到输出。因此，实际上有两个隐藏层：共享单词特征层C，其中没有非线性（这不会添加任何有用的内容），以及普通的双曲正切隐藏层。更准确地说，神经网络计算以下函数，具有softmax输出层，可以确保概率为1的正值和：
Pˆ(w tjw t−1,(cid:1)(cid:1)(cid:1)w t−n+1)=
(cid:229)
e iy ewt
yi。
3.偏差是神经网络的加性参数，例如下面的方程式（1）中的b和d。

---

神经概率语言模型
它们是每个输出单词i的未经归一化的对数概率，如下所计算，具有参数b、W、U、d和H：
y=b+Wx+Utanh(d+Hx) (1)
其中双曲正切tanh是逐元素应用的，W可选为零（无直接连接），x是单词特征层激活向量，是来自矩阵C的输入词特征的连接：
x=(C(w t−1),C(w t−2),...C(w t−n+1)).
设h为隐藏单元的数量，m为与每个单词相关联的特征数量。当不需要从单词特征到输出的直接连接时，将矩阵W设置为0。模型的自由参数是输出偏差b（具有jVej个元素）、隐藏层偏差d（具有h个元素）、隐藏到输出权重U（ajVj×hmatrix）、单词特征到输出权重W（ajVj×(n−1)m矩阵）、隐藏层权重H（ah×(n−1)m矩阵）和单词特征C（ajVj×m矩阵）：
q =(b,d,W,U,H,C).
自由参数的数量为jVj(1+nm+h)+h(1+(n−1)m)。主导因素是 jVj(nm+h)。请注意，理论上，如果权重W和H上有权重衰减而C上没有，那么W和H可能会收敛至零，而C会不断增大。在实践中，我们在使用随机梯度上升训练时并未观察到这种行为。
神经网络上的随机梯度上升是在呈现训练语料库的第t个单词后执行以下迭代更新：
q q +e
¶ logPˆ(w tjw t−1,...w t−n+1)
¶q
其中e是“学习率”。注意，大部分参数在每个示例后无需更新或访问：所有不在输入窗口中出现的单词j的单词特征C(j)。
混合模型。在我们的实验中（见第4节），通过将神经网络的概率预测与插值三元模型的概率预测相结合，可以获得改进的性能，可以使用简单的固定权重0.5，学习权重（在验证集上最大似然）或一组与上下文频率有关的权重（使用与组合三元模型、二元模型和一元模型相同的程序在插值三元模型中组合）。
并行实现
尽管参数数量与输入窗口的大小线性缩放（与词汇量大小线性缩放），但获取输出概率所需的计算量远远大于n-gram模型所需的计算量。主要原因是，对于n-gram模型，获取特定的P(w tjw t−1,...,w t−n+1)不需要计算词汇表中所有单词的概率，这是由于线性组合相对频率时容易进行的归一化（在训练模型时执行）。神经实现的主要计算瓶颈是计算输出层的激活。

---

BENGIO，DUCHARME，VINCENT和JAUVIN在并行计算机上运行模型（包括训练和测试）是减少计算时间的一种方法。我们已经探索了两种类型的平台上的并行化：共享内存处理器机器和具有快速网络的Linux集群。
3.1 数据并行处理
在共享内存处理器的情况下，由于处理器之间的通信开销非常低，通过共享内存很容易实现并行化。在这种情况下，我们选择了数据并行实现，每个处理器在不同的数据子集上工作。每个处理器计算其示例的梯度，并对模型的参数执行随机梯度更新，这些参数简单地存储在共享内存区域中。我们的第一个实现非常慢，并且依赖于同步命令，以确保每个处理器不会与上述参数子集中的另一个处理器同时写入。每个处理器的大多数周期都花在等待另一个处理器释放对参数写入访问锁的时间上。
相反，我们选择了一个异步实现，其中每个处理器可以随时在共享内存区域中写入。有时，一个处理器对参数向量的更新的一部分会丢失，被另一个处理器的更新覆盖，这会在参数更新中引入一点噪音。然而，这种噪音似乎非常小，并且没有明显减慢训练速度。
不幸的是，大型共享内存并行计算机非常昂贵，它们的处理器速度往往落后于可以连接成集群的主流CPU。因此，我们已经能够在快速网络集群上获得更快的训练速度。
3.2 参数并行处理
如果并行计算机是多个CPU组成的网络，我们通常无法经常在处理器之间交换所有参数，因为这代表着数十兆字节（在我们最大的网络的情况下几乎达到100兆字节），通过局域网传输需要太长时间。相反，我们选择了跨参数进行并行化，特别是输出单元的参数，因为我们的架构中绝大部分计算发生在那里。每个CPU负责计算一部分输出的非归一化概率，并执行对应输出单元参数（进入该单元的权重）的更新。这种策略使我们能够进行带有可忽略通信开销的并行随机梯度上升。CPU基本上需要传达两个信息：（1）输出softmax的归一化因子，以及（2）隐藏层（表示为a）和单词特征层（表示为x）的梯度。所有CPU都复制出现在输出单元激活计算之前的计算，即单词特征的选择和隐藏层激活a的计算，以及相应的反向传播和更新步骤。然而，这些计算在我们的网络的总计算中只占很小一部分。
例如，考虑在AP（美联社）新闻数据实验中使用的以下架构：词汇量为|V|=17,964，隐藏单元数为h=60，模型的阶数为n=6，单词特征数为m=100。处理单个训练示例的总数值运算次数约为|V|(1+nm+h)+h(1+nm)+nm（其中术语分别对应于计算输出单元、隐藏单元和单词特征的计算）。

---

一个神经概率语言模型
特征单元）。在这个例子中，用于计算输出单元加权总和所需的整体计算的比例约为jVj(1+(n−1)m+h) = jVj(1+(n−1)m+h)+h(1+(n−1)m)+(n−1)m99.7%。这个计算是近似的，因为不同操作相关的实际CPU时间是不同的，但这表明并行计算输出单元通常是有利的。所有CPU都会复制计算的非常小的一部分事实不会影响这里所寻求的并行化级别的总计算时间，即几十个处理器。如果隐藏单元的数量很大，那么并行化它们的计算也会变得有利，但我们在实验中没有调查这种方法。
这种策略的实施是在一组1.2GHz时钟速度的Athlon处理器（32x2CPU）上完成的，通过Myrinet网络（低延迟千兆本地区域网络）连接，使用MPI（消息传递接口）库（Dongarra等人，1995）进行并行化例程。下面概述了这种并行化算法，用于由M个处理器集群中的CPU i并行执行的单个示例（wt−n+1，(cid:1)(cid:1)(cid:1)，wt）。CPU i（i范围从0到M−1）负责从编号start=i(cid:2)djVj/Me开始的一块输出单元，该块的长度为min(djVj/Me,jVj−start)。
对于处理器i的计算，示例t
1. 前向阶段
(a) 对单词特征层执行前向计算：
x(k) C(w t−k),
x=(x(1),x(2),(cid:1)(cid:1)(cid:1),x(n−1))
(b) 对隐藏层执行前向计算：
o d+Hx
a tanh(o)
(c) 对第i个块中的输出单元执行前向计算：
s 0
循环遍历第i块中的j
i. y b +a.U
ii. 如果（直接连接） y y +x.W
iii. p
eyj
iv. s s +p
计算并共享S=(cid:229) s 之间的处理器。这可以很容易地通过MPI Allreduce操作实现，该操作可以高效地计算并分享这个和。
(e) 规范化概率：
遍历第i块中的j，p p /S。
(f) 更新对数似然。 如果w落在CPU i>0的块中，则CPU i将p发送给CPU 0。 CPU 0计算L=logp并跟踪总对数似然。
2. 后向/更新阶段，学习率e。
(a) 对第i块中的输出单元执行反向梯度计算：
清除梯度向量 ¶ L 和 ¶ L。
¶ a ¶ x
循环遍历第i块中的j

---

BENGIO，DUCHARME，VINCENT ANDJAUVIN
i. ¶¶ yL
j
1 j==wt − p j
ii. b b +e ¶ L
j j ¶ yj
如果（直连）¶ L ¶ L+ ¶ LW
¶ x ¶ x ¶ yj j
¶ L ¶ L+ ¶ LU
¶ a ¶ a ¶ yj j
如果（直连）W W +e ¶ Lx
j j ¶ yj
U U +e ¶ La
j j ¶ yj
（b）对处理器之间的 ¶ L 和 ¶ L 汇总和共享。可以通过MPI Allreduce操作轻松实现。
（c）通过和更新隐藏层权重进行反向传播：
循环k从1到h，
¶ L (1−a2)¶ L
¶ ok k ¶ ak
¶ L ¶ L+H0¶ L
¶ x ¶ x ¶ o
d d+e ¶ L
¶ o
H H+e ¶ Lx0
¶ o
（d）更新输入单词的词特征向量：
循环k从1到n−1
C(w t−k) C(w t−k)+e
¶
x¶ (L
k)
其中 ¶ L 是向量 ¶ L 的第k个块（长度为m）。
¶ x(k) ¶ x
在上述实现中未显示权重衰减正则化，但可以很容易地加入（通过从每个参数中减去
权重衰减因子乘以学习率乘以参数值的结果，在每次更新时）。请注意，参数更新是直接
进行的，而不是通过参数梯度向量，以增加速度，计算速度的限制因素是内存访问，在我们
的实验中。
在前向阶段计算指数时可能存在数字问题，所有p都可能在数字上为零，或其中一个可能
对计算指数过大（见步骤1(c)ii）。为了避免这个问题，通常的解决方案是在softmax中取
指数之前先减去y 的最大值。因此，我们增加了一个额外的Allreduce操作，来在
j
处理器之间共享y 中的最大值，在计算指数之前。设q是第i块中y 的最大值。然后
j
总体最大值Q= max q被共同计算并在M个处理器之间共享。然后计算指数如下：
i i
p
eyj−Q
（而不是步骤1(c)ii），以确保至少一个p是数字非零，并且指数参数的最大值为1。
通过将并行版本的时钟时间与单个处理器的时钟时间进行比较，我们发现通信开销
仅占总时间的1/15（对于一个训练时期）：因此，通过在快速网络上使用该算法进行并行
化，我们获得了几乎完美的加速。在网络速度较慢的集群上，可能仍然可以通过每K个示例
（一个小批次）执行通信来获得有效的并行化。这需要在每个处理器中存储神经网络的激活
和梯度的K个版本。在K个示例的前向阶段之后，概率总和必须在处理器之间共享。

---

神经概率语言模型处理器。然后启动K个向后阶段，以获得K个部分梯度向量¶ L
¶ a和¶ L。在处理器之间交换这些梯度向量后，每个处理器可以完成向后阶段并更新参数。这种方法主要节省时间，因为减少了网络通信延迟（传输的数据量是相同的）。如果K值过大，收敛时间可能会损失，原因与批梯度下降通常比随机梯度下降慢很多相同（LeCun等，1998年）。
4.实验结果
对Brown语料库进行了比较实验，该语料库是包含各种英语文本和书籍的1,181,041个单词的流。前80万单词用于训练，接下来的20万用于验证（模型选择，权重衰减，提前停止），剩下的181,041用于测试。不同的单词数量为47,578（包括标点符号，区分大小写，以及用于分隔文本和段落的句法标记）。频率为（cid：20）3的罕见单词合并为一个符号，将词汇量大小减少到jVj=16,383。
还对1995年和1996年的美联社（AP）新闻文本进行了实验。训练集包含约1400万（13,994,528）个单词的流，验证集包含约100万（963,138）个单词的流，测试集也包含约100万（963,071）个单词的流。原始数据有148,721个不同的单词（包括标点符号），通过仅保留最常见的单词并保留标点符号，将其减少到jVj=17964，将大写映射为小写，将数字形式映射到特殊符号，将罕见单词映射为特殊符号，并将专有名词映射为另一个专用符号。
对于训练神经网络，初始学习率设置为e = 10^-3（经过几次试验使用了一个微小数据集），并根据以下时间表逐渐降低：e = e o，其中t表示完成的参数更新次数，r是一种经验减少因子，被选择为r=10^-8。
4.1 N-Gram模型
神经网络与之进行比较的第一个基准是插值或平滑的三元模型（Jelinek和Mercer，1980）。让q
t
= l（freq（w t−1，w t−2））表示输入上下文（w t−1，w t−2）的离散发生频率。然后，条件概率估计具有以下形式的条件混合：
Pˆ（w tjw t−1，w t−2）=a 0（q t）p 0+a 1（q t）p 1（w t）+a 2（q t）p 2（w tjw t−1）+a 3（q t）p 3（w tjw t−1，w t−2）
其中条件权重a（q）(cid:21) 0,(cid:229) a（q ）= 1。基本预测器如下：p=
i t i i t 0
1/jVj，p（i）是一元（训练集中单词i的相对频率），p（ijj）是二元（前一个词为j时，单词i的相对频率），而p（ijj，k）是三元（前两个词为j和k时，单词i的相对频率）。动机是当（w t−1，w t−2）的频率很大时，p
3
是最可靠的，而当它较低时，p，甚至p的较低阶统计量p
2 1 0
更可靠。每个离散值q（上下文频率箱）都有一个不同的混合权重a，它们可以通过t
4.我们使用了l（x）=d−log（（1+x）/T），其中freq（w t−1，w t−2）是输入上下文的出现频率，T是训练语料库的大小。很容易估计。

---

BENGIO、DUCHARME、VINCENT和ANDJAUVIN在大约5次迭代中，使用一组未用于估计unigram、bigram和trigram相对频率的数据集（验证集）上应用了EM算法。插值的n-gram用于与MLP混合，因为它们似乎以非常不同的方式产生“错误”。

还与其他最先进的n-gram模型进行了比较：带有Modified Kneser-Ney算法（Kneser和Ney，1995，Chen和Goodman，1999）的回退n-gram模型，以及基于类的n-gram模型（Brown等，1992，Ney和Kneser，1993，Niesler等，1998）。验证集用于选择n-gram的顺序和基于类的模型的单词类别数。我们使用了SRILanguage Modeling工具包中Stolcke（2002）描述的算法实现，网址为www.speech.sri.com/projects/srilm/。它们用于计算下面报告的回退模型困惑度，需要注意的是，我们没有特别区分完整的句子标记在对数似然度量的计算中，正如我们的神经网络困惑度一样。所有标记（单词和标点符号）在计算对数似然度时都被视为相同（因此也会得到困惑度）。

4.2 结果
下面是不同模型测试集困惑度（1/Pˆ(w jwt−1)的几何平均）的度量结果。对于Brown语料库，随机梯度上升程序在大约10到20个epochs后显示了明显的收敛。在AP新闻语料库中，我们没有看到过拟合的迹象（在验证集上），可能是因为我们只运行了5个epochs（使用了40个CPU运行了3周）。在Brown实验中只有在必要时才使用了早停技术。Brown实验中使用了10^-4的权重衰减惩罚项，而AP新闻实验中使用了10^-5的权重衰减（通过几次试验后基于验证集困惑度选取的）。

表1总结了在Brown语料库上获得的结果。此表中的所有回退模型都是修改后的Kneser-Ney n-gram模型，比标准回退模型表现显著更好。表中指定m的回退模型时，使用了基于类的n-gram（m是单词类别数量）。初始化单词特征采用了随机初始化（类似于神经网络权重的初始化），但我们认为通过基于知识的初始化可能会取得更好的结果。

主要结果是，与最好的n-gram相比，使用神经网络可以取得显著更优异的结果，在Brown语料库上有约24%的测试困惑度差异，而在AP新闻上有约8%的差异，当使用在验证集上表现最好的MLP和n-gram时。表还表明神经网络能够利用更多的上下文（在Brown上，从2个单词的上下文扩展到4个单词会改善神经网络，但不会改善n-gram）。它还显示了隐藏单元的用处（MLP3对比MLP1和MLP4对比MLP2），以及将神经网络的输出概率与插值trigram混合总是有助于减少困惑度。简单平均有所帮助表明神经网络和trigram在不同的地方会出现错误（即对观察到的单词给出低概率）。结果并不允许得出直接从输入到输出的连接是否有用的结论，但建议在较小的语料库中至少可以获得更好的泛化能力而不需要直接输入到输出的连接，尽管训练时间更长：没有直接连接时，网络收敛需要的时间是原来的两倍（20次epoch而不是10），尽管困惑度稍微降低。一个合理的解释是，直接输入到输出的连接提供了更多的容量并更快地学习从单词特征到对数的“线性”映射。

---

一种神经概率语言模型

MLP1 5 50 60 是 否 182 284 268
MLP2 5 50 60 是 是 275 257
MLP3 5 0 60 是 否 201 327 310
MLP4 5 0 60 是 是 286 272
MLP5 5 50 30 是 否 209 296 279
MLP6 5 50 30 是 是 273 259
MLP7 3 50 30 是 否 210 309 293
MLP8 3 50 30 是 是 284 270
MLP9 5 100 30 否 否 175 280 276
MLP10 5 100 30 否 是 265 252
Del. Int. 3 31 352 336
Kneser-Neyback-off 3 334 323
Kneser-Neyback-off 4 332 321
Kneser-Neyback-off 5 332 321
基于类别的回退 3 150 348 334
基于类别的回退 3 200 354 340
基于类别的回退 3 500 326 312
基于类别的回退 3 1000 335 319
基于类别的回退 3 2000 343 326
基于类别的回退 4 500 327 312
基于类别的回退 5 500 327 312

表1：布朗语料库的结果比较。删除插值三元组的困惑度比具有最低验证困惑度的神经网络高33%。
在最佳n-gram情况下（一个具有500个词类的基于类别的模型），差异为24%。
n：模型的阶数。c：基于类别的n-gram中的词类数。h：隐藏单元数量。m：MLP的字特征数量，基于类别的n-gram的类别数量。direct：是否存在从字特征到输出的直接连接。mix：神经网络的输出概率是否与三元组的输出混合（每个重量为0.5）。最后三列给出训练、验证和测试集的困惑度。概率。另一方面，没有这些连接，隐藏单元将形成紧密的瓶颈，可能导致更好的泛化。

表2在更大的语料库（AP新闻）上给出类似的结果，尽管困惑度有所降低（8％）。仅进行了5个epoch（大约三周时间，使用40个CPU）。在这种情况下，基于类别的模型似乎并没有帮助n-gram模型，但高阶修改的Kneser-Neyback-off模型在n-gram模型中的表现最好。

5. 扩展和未来工作
在本节中，描述了对上述模型的扩展和未来工作方向。

---

BENGIO、DUCHARME、VINCENT ANDJAUVIN
n h m 直接混合 训练 验证 测试
MLP10 6 60 100 是 是 104 109
Del. Int. 3 126 132
退避KN 3 121 127
退避KN 4 113 119
退避KN 5 112 117
表2：AP新闻语料库的比较结果。有关列标签，请参见前表。
5.1 能量最小化网络
上述神经网络的变体可以解释为能量最小化模型，这是基于 Hinton 最近关于专家组的产品的工作（Hinton，2000）。在前面部分描述的神经网络中，分布式单词特征仅用于“输入”单词，而不用于“输出”单词（下一个单词）。此外，输出层扩展了非常大量的参数（大多数），输出单词之间的语义或语法相似性没有被利用。在此描述的变体中，输出单词也由其特征向量表示。网络输入单词子序列（映射到它们的特征向量）并输出一个能量函数 E，在单词形成可能的子序列时低，当不可能时高。例如，网络输出一个“能量”函数

E(w t−n+1,(cid:1)(cid:1)(cid:1),w t)=v.tanh(d+Hx)+(cid:229) b
w t−i
i=0
其中 b 是偏置向量（对应于无条件概率），d 是隐藏单元偏置向量，v 是输出权重向量，H 是隐藏层权重矩阵，不同于前一模型，输入和输出单词共同贡献于 x：

x=(C(w t),C(w t−1),C(w t−2),(cid:1)(cid:1)(cid:1),C(w t−n+1)。

能量函数 E(w t−n+1,(cid:1)(cid:1)(cid:1),w t) 可以解释为(w t−n+1,(cid:1)(cid:1)(cid:1),w t) 的未归一化对数概率。要获得条件概率 Pˆ(w t|w t−1,...,w t−n+1) 只需在可能的 w 值上进行归一化，如下：

Pˆ(w t|w t−1,...,w t−n+1)= e−E(w t−n+1,(cid:1)(cid:1)(cid:1),w t) / Σ i e−E(w t−n+1,(cid:1)(cid:1)(cid:1),w t−1,i)

请注意，总计算量与之前介绍的架构相当，并且如果 v 参数由目标词（w ）的身份索引，则参数数量也可以匹配。请注意，上述 softmax 归一化后仅保留 b（任何 w t−i 的线性函数对 i>0 被 softmax 归一化取消）。与之前一样，通过对 logPˆ(w t|w t−1,...,w t−n+1) 的随机梯度上升进行类似计算可以调整模型参数。在专家组的产品框架中，隐藏单元可被视为专家：子序列 (w t−n+1,(cid:1)(cid:1)(cid:1),w t) 的联合概率与与每个隐藏单元 j 相关的术语的总和的指数成正比，v tanh(d +H x)。请注意，因为我们选择将整个序列的概率分解为每个元素的条件概率，所以
总体概率的计算量与之前的架构相当。

---

神经概率语言模型

梯度的计算是可行的。例如，在HMMs的乘积中（Brown和Hinton，2000），梯度的计算就不是这样的情况，其中的乘积是对查看整个序列的专家进行的，可以使用对比散度算法（Brown和Hinton，2000）等近似梯度算法进行训练。还要注意，这种结构和专家乘积的制定可以看作是非常成功的最大熵模型（Berger等，1996）的扩展，但这里的基础函数（或“特征”，这里指隐藏单元的激活）是通过惩罚最大似然学习的方式同时学习的，而不是通过贪婪的特征子集选择方法在外部环节学习。

我们已经实现并对上述结构进行了实验，并开发了一种神经网络训练加速技术，基于重要性抽样，可以实现100倍的加速（Bengio和Senécal，2003）。

未登录词。这种结构相对于前一种结构的优势是可以很容易地处理未登录词（甚至可以对其赋予概率）。主要思想是首先猜测这样一个词的初始特征向量，通过取可能出现在相同上下文中的其他词的特征向量的加权凸组合，权重与条件概率成正比。假设网络分配了一个概率Pˆ(ijwt−1)给上下文wt−1中的词i2V，并且在这个上下文中我们观察到一个新词j62V。我们将词j的特征向量C(j)初始化如下：C(j) = i2V C(i)Pˆ(ijwt t−1 −n+1)。然后我们可以将j纳入V并重新计算这个稍大的集合的概率（这只需要对所有单词进行归一化，除了单词i需要通过神经网络进行一次传递）。这个特征向量C(i)然后可以在尝试预测接下来的单词的概率时用于输入上下文部分。

未来的其他工作

在这项工作之后仍然有许多挑战需要解决。在短期内，需要设计和评估加速训练和识别方法。在长期内，需要介绍更多推广方法，除了这里利用的两种主要方法。以下是一些我们打算探索的想法：
1. 将网络拆分为子网络，例如使用单词的聚类。训练许多较小的网络应该更容易更快。
2. 用树结构表示条件概率，其中在每个节点应用一个神经网络，每个节点表示给定上下文的单词类别的概率，叶子表示给定上下文的单词的概率。这种表示方式有可能通过jVj/logjVj的因子来减少计算时间（参见Bengio，2002）。
3. 仅从输出单词的子集传播梯度。可以是条件上最有可能的单词（基于一个更快的模型，比如三元模型，参见Schwenk和Gauvain，2002，用于此思想的应用），或者可以是在三元模型表现不佳的单词子集。如果语言模型与语音识别器耦合，那么只需要计算声学上模糊的单词的得分（未归一化概率）。请参阅Bengio和Senécal（2003）了解一种使用重要性抽样选择单词的新加速训练方法。

---

介绍先验知识。可以引入多种形式的知识，例如：语义信息（例如从WordNet获取，参见Fellbaum，1998）、低级别语法信息（例如使用词性），以及高级别语法信息，例如将模型与随机语法耦合，如Bengio（2002）所建议的。引入更多的结构和参数共享来捕捉长期上下文的影响，例如使用时滞或循环神经网络。在这样一个多层网络中，对一小组连续单词进行的计算在网络输入窗口移动时不需要重新进行。同样地，人们可以使用循环网络来捕捉有关文本主题的潜在更长期信息。

解释（并可能使用）神经网络学习的单词特征表示。一个简单的第一步可能是从m=2个特征开始，这样更容易显示。我们认为，要获得更有意义的表示需要大规模的训练语料库，特别是对于更大的m值。

这里提出的模型可能不适用于多义词，它将每个单词分配给连续语义空间中的一个点。我们正在研究此模型的扩展，其中每个单词与该空间中的多个点相关联，每个点与单词的不同意义相关联。

结论
对两个语料库进行的实验，其中一个有一百万个例子，另一个有超过一千五百万个单词，表明所提出的方法比现有技术的平滑三元组方法在困惑度方面要好得多，困惑度相差10%到20%。我们认为这些改进的主要原因是所提出的方法允许利用学习到的分布式表示来应对维度的诅咒，每个训练句子通知模型有关其他大量句子的组合数。

在模型的改进方面可能有很多工作要做，包括架构、计算效率，以及利用先验知识。未来研究的一个重要优先事项应该是改进加速技术，以及找到增加容量而不会增加太多训练时间的方法（以处理含有数亿甚至更多单词的语料库）。一个简单的想法是利用时间结构并扩大输入窗口的大小，可能包括整段文字（而不会增加太多参数或计算时间），是使用时间滞和可能循环的神经网络。对这里提出的模型在应用背景下的评估也将很有用，但请参考Schwenk和Gauvain（2002）已完成的工作，用于改善语音识别单词错误率。

更广泛地说，此处提出的工作为用分布表示替换“条件概率表”带来的统计语言模型的改进打开了大门，这种表示可以容纳更多的调节变量，远远超过以往。尽管在统计语言模型（例如随机语法）中已经花费了很多精力来限制或汇总调节变量，以避免过度拟合，但是现在可以通过分布表示来实现更紗和平滑的表征。Bengio和Senécal（2003）提出了一种百倍加速技术。

---

神经概率语言模型通过在其他地方转移困难：需要更多计算，但计算和内存需求与条件变量的数量成线性关系，而不是指数关系。

致谢
作者要感谢Léon Bottou、Yann LeCun和Geoffrey Hinton的有益讨论。此研究得到NSERC资助机构以及MITACS和IRIS网络的资助。

参考文献
D.Baker和A.McCallum. Distributional clustering of words for text classification. In SIGIR’98, 1998.
J.R. Bellegarda. A latent semantic analysis framework for large-span language modeling. In Proceedings of Eurospeech 97, pages 1451–1454, Rhodes, Greece, 1997.
S. Bengio和Y. Bengio. 使用神经网络处理联合分布中的维度灾难。IEEE神经网络交易，特刊数据挖掘与知识发现，11(3):550–557, 2000a.
Y. Bengio. 新的分布式概率语言模型。技术报告1215，蒙特利尔大学IRO系，2002.
Y. Bengio和S. Bengio. 使用多层神经网络建模高维离散数据。在S. A. Solla、T. K. Leen和K-R. Müller编辑的《神经信息处理系统进展》中，卷12，页400–406。MIT Press，2000b.
Y. Bengio和J-S.Senécal. 通过重要性抽样快速训练概率神经网络。在AISTATS，2003年。
A. Berger，S. Della Pietra和V. Della Pietra. 自然语言处理的最大熵方法。计算语言学，22:39–71, 1996.
A. Brown和G.E. Hinton. 隐马尔科夫模型的乘积。技术报告GCNU TR 2000-004，Gatsby Unit，伦敦大学学院，2000.
P.F.Brown,V.J.DellaPietra,P.V.DeSouza,J.C.Lai和R.L.Mercer. 自然语言的基于类的n-gram模型。计算语言学，18:467–479, 1992.
S.F.Chen和J.T.Goodman. 关于语言建模的平滑技术的实证研究。计算机、语音和语言，13(4):359–393, 1999.
S. Deerwester，S.T. Dumais，G.W. Furnas，T.K. Landauer和R. Harshman. 潜在语义分析检索。美国信息科学协会期刊，41(6):391–407, 1990.
J. Dongarra，D. Walker和消息传递接口论坛. MPI: 消息传递接口标准。技术报告 http://www-unix.mcs.anl.gov/mpi，田纳西大学，1995。

---

BENGIO, DUCHARME, VINCENT ANDJAUVIN
J.L.Elman. Findingstructureintime. 认知科学，14:179–211，1990年。
C.Fellbaum. WordNet：一个电子词汇数据库。 MIT出版社，1998年。
J.Goodman. 语言建模中的一点进展。 MSR-TR-2001-72技术报告，微软研究，2001年。
G.E. Hinton. 学习概念的分布式表示。在第八届认知科学协会年会论文集中，第1-12页，1986年，1986年。 劳伦斯厄尔鲍姆，希尔兹代尔。
G.E.Hinton. 通过最小化对比离散来训练专家产品。 GatsbyUnit技术报告GCNU-TR 2000-004,伦敦大学学院，2000年。
F.Jelinek和R.L.Mercer. 从稀疏数据中插值估计马尔可夫源参数。在E.S.Gelsema和L.N.Kanal，编辑，实践中的模式识别。北荷兰，阿姆斯特丹，1980年。
K.J.Jensen和S.Riis. 用于文本到语音神经网络模型的自组织字母码本。 在ICSLP 2000年会议文集中。
S.M. Katz. 从稀疏数据中估计语音识别器的语言模型组件的概率。 IEEE声学，语音和信号处理交易，ASSP-35（3）：400-401，1987年3月。
R.Kneser和H.Ney. 改进的m-gram语言建模倒退。 在国际声学，语音和信号处理会议上，1995年，第181-184页。
Y.LeCun，L.Bottou，G.B.Orr和K.-R.Müller. 高效的反向传播。 在神经网络：行业内幕中，9-50页。 Springer，1998年。
R.Miikkulainen和M.G.Dyer. 模块化神经网络和分布式词典的自然语言处理。 认知科学，15:343-399，1991年。
H.Ney和R.Kneser. 用于基于类的统计语言建模的改进聚类技术。 在欧洲语音通信和技术会议（Eurospeech）上，1993年，柏林，第973-976页。
T.R.Niesler，E.W.D.Whittaker和P.C.Woodland. 词性和自动派生基于类别的语言模型与语音识别的比较。 在声学，语音和信号处理国际会议上，第177-180页，1998年。
A.Paccanaro和G.E.Hinton. 从正面和负面命题中提取概念和关系的分布表示。 在国际联合神经网络会议IJCNN’2000论文集中，科莫，意大利，2000年。 IEEE，纽约。
F.Pereira，N.Tishby和L.Lee. 英语单词的分布聚类。 在第30届计算语言学协会年会上，第183-190页，哥伦布，俄亥俄州，1993年。

---

神经概率语言模型
S. Riis和A. Krogh。使用结构化神经网络和多个序列配置改进蛋白质二级结构预测。计算生物学杂志，163-183页，1996年。
J.Schmidhuber。顺序神经文本压缩。IEEE神经网络交易，7（1）：142-146，1996年。
H.Schutze。词空间。在S.J.Hanson，J.D.Cowan和C.L.Giles，编辑，《神经信息处理系统进展5》，895-902页，1993年圣马特奥加，摩根考夫曼。
H. Schwenk和J-L. Gauvain。用于大词汇连续语音识别的连接主义语言建模。在国际声学、语音和信号处理会议上，765-768页，佛罗里达奥兰多，2002年。
A.Stolcke。SRILM-一个可扩展的语言建模工具包。在国际统计语言处理会议论文集中，丹佛，科罗拉多，2002年。
W. Xu和A. Rudnicky。人工神经网络能否学习语言模型。在国际统计语言处理会议上，M1-13页，中国北京，2000年。

